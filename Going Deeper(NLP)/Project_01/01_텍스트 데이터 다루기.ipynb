{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44017448",
   "metadata": {},
   "source": [
    "## 전처리: 자연어의 노이즈 제거"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97797df1",
   "metadata": {},
   "source": [
    "### 1) 문장부호\n",
    "- Hi, 가 Hi 와 , 의 결합인 것을 알지만 컴퓨터는 명시해 주지 않는다면 알파벳에 , 가 포함되어 있다고 생각할 수 있음\n",
    "- 문장부호를 단어와 분리하면 해결이 되는 상황이기 때문에 문장부호 양쪽에 공백을 추가하는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72fe582f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi ,  my name is john . \n"
     ]
    }
   ],
   "source": [
    "def pad_punctuation(sentence, punc):\n",
    "    for p in punc:\n",
    "        sentence = sentence.replace(p, \" \" + p + \" \")\n",
    "    return sentence\n",
    "\n",
    "sentence = \"Hi, my name is john.\"\n",
    "\n",
    "print(pad_punctuation(sentence, [\".\", \"?\", \"!\", \",\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42345aea",
   "metadata": {},
   "source": [
    "### 2) 대소문자\n",
    "- First와 first 는 같은 의미를 갖고 있음에도 컴퓨터는 f와 F를 다르다고 구분지을 수 있음\n",
    "- 모든 단어를 소문자로 바꾸는 방법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "953f7c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first, open the first chapter.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35173f23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FIRST, OPEN THE FIRST CHAPTER.\n"
     ]
    }
   ],
   "source": [
    "sentence = \"First, open the first chapter.\"\n",
    "\n",
    "print(sentence.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e193d6",
   "metadata": {},
   "source": [
    "### 3) 특수문자\n",
    "- re 패키지(정규표현식 사용을 도와주는 패키지)를 이용하여 처리\n",
    "- ten-year-old와 seven-year-old와 같은 표현들 사이에 있는 '-'를 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9590d52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He is a ten year old boy.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "sentence = \"He is a ten-year-old boy.\"\n",
    "sentence = re.sub(\"([^a-zA-Z.,?!])\", \" \", sentence)\n",
    "\n",
    "print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbc7429b",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "10313872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
      "but my teacher had been with me several weeks before i understood that everything has a name . \n",
      "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
      "some one was drawing water and my teacher placed my hand under the spout .  \n",
      "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
      "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
      "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
      "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
      "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
      "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From The Project Gutenberg\n",
    "# (https://www.gutenberg.org/files/2397/2397-h/2397-h.htm)\n",
    "\n",
    "corpus = \\\n",
    "\"\"\"\n",
    "In the days that followed I learned to spell in this uncomprehending way a great many words, among them pin, hat, cup and a few verbs like sit, stand and walk. \n",
    "But my teacher had been with me several weeks before I understood that everything has a name.\n",
    "One day, we walked down the path to the well-house, attracted by the fragrance of the honeysuckle with which it was covered. \n",
    "Some one was drawing water and my teacher placed my hand under the spout. \n",
    "As the cool stream gushed over one hand she spelled into the other the word water, first slowly, then rapidly. \n",
    "I stood still, my whole attention fixed upon the motions of her fingers. \n",
    "Suddenly I felt a misty consciousness as of something forgotten—a thrill of returning thought; and somehow the mystery of language was revealed to me. \n",
    "I knew then that \"w-a-t-e-r\" meant the wonderful cool something that was flowing over my hand. \n",
    "That living word awakened my soul, gave it light, hope, joy, set it free! \n",
    "There were barriers still, it is true, but barriers that could in time be swept away.\n",
    "\"\"\" \n",
    "\n",
    "def cleaning_text(text, punc, regex):\n",
    "    # 노이즈 유형 (1) 문장부호 공백추가\n",
    "    for p in punc:\n",
    "        text = text.replace(p, \" \" + p + \" \")\n",
    "\n",
    "    # 노이즈 유형 (2), (3) 소문자화 및 특수문자 제거\n",
    "    text = re.sub(regex, \" \", text).lower()\n",
    "\n",
    "    return text\n",
    "\n",
    "print(cleaning_text(corpus, [\".\", \",\", \"!\", \"?\"], \"([^a-zA-Z0-9.,?!\\n])\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac651cda",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b4f475a",
   "metadata": {},
   "source": [
    "## 분산표현(distributed representation) \n",
    "- 희소 표현과는 반대되는 의미를 가짐\n",
    "    - 희소 표현은 단어를 고정된 크기의 벡터로 표현하지 않고, 이진화 또는 빈도수의 방식으로 표현하는 방식\n",
    "    - 단어의 존재 유무를 나타내며, 벡터 공간 상의 거리 측정, 단어 간의 의미 관계 파악이 어려움\n",
    "- 분산 표현은 하나의 단어를 여러 차원의 값으로 나타낸 것\n",
    "- 단어 간의 거리 측정을 통해 단어 간의 의미의 관련성을 파악할 수 있음\n",
    "- Embedding 레이어를 사용해서 각 단어가 몇 차원의 속성을 가질지 정의하는 방식으로 분산 표현을 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6f3ed79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 만약 100개의 단어를 256차원의 속성으로 표현하고 싶다면\n",
    "import tensorflow as tf\n",
    "\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=100, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9badfc",
   "metadata": {},
   "source": [
    "### 코사인 유사도(Cosine Similarity)\n",
    "- 워드 벡터끼리는 단어들 간의 의미적 유사도를 계산\n",
    "- 희소 표현을 보이기 위해 코사인 유사도를 사용해 봄"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6aaabde8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_1과 word_2의 유사도 : 0.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cos_sim(A, B):\n",
    "    return dot(A, B)/(norm(A)*norm(B))\n",
    "\n",
    "word_1 = np.array([-1.0, 0.0, 0.0, 0.0])\n",
    "word_2 = np.array([0.0, 0.0, 1.0, 0.5])\n",
    "\n",
    "print('word_1과 word_2의 유사도 :',cos_sim(word_1, word_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59d9a37",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa04091",
   "metadata": {},
   "source": [
    "## 토큰화\n",
    "- 토큰(Token) : 문장을 어떤 기준으로 쪼개었을 때, 쪼개진 각 단어들을 의미\n",
    "- 쪼개진 기준이 토근화(Tokenization)기법에 의해 정해짐"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76abb54f",
   "metadata": {},
   "source": [
    "### 1) 공백 기반 토큰화\n",
    "- 말그대로 공백을 기반으로 토큰화를 진행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2f7d96b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "문장이 포함하는 Tokens: ['in', 'the', 'days', 'that', 'followed', 'i', 'learned', 'to', 'spell', 'in', 'this', 'uncomprehending', 'way', 'a', 'great', 'many', 'words', ',', 'among', 'them', 'pin', ',', 'hat', ',', 'cup', 'and', 'a', 'few', 'verbs', 'like', 'sit', ',', 'stand', 'and', 'walk', '.', 'but', 'my', 'teacher', 'had', 'been', 'with', 'me', 'several', 'weeks', 'before', 'i', 'understood', 'that', 'everything', 'has', 'a', 'name', '.', 'one', 'day', ',', 'we', 'walked', 'down', 'the', 'path', 'to', 'the', 'well', 'house', ',', 'attracted', 'by', 'the', 'fragrance', 'of', 'the', 'honeysuckle', 'with', 'which', 'it', 'was', 'covered', '.', 'some', 'one', 'was', 'drawing', 'water', 'and', 'my', 'teacher', 'placed', 'my', 'hand', 'under', 'the', 'spout', '.', 'as', 'the', 'cool', 'stream', 'gushed', 'over', 'one', 'hand', 'she', 'spelled', 'into', 'the', 'other', 'the', 'word', 'water', ',', 'first', 'slowly', ',', 'then', 'rapidly', '.', 'i', 'stood', 'still', ',', 'my', 'whole', 'attention', 'fixed', 'upon', 'the', 'motions', 'of', 'her', 'fingers', '.', 'suddenly', 'i', 'felt', 'a', 'misty', 'consciousness', 'as', 'of', 'something', 'forgotten', 'a', 'thrill', 'of', 'returning', 'thought', 'and', 'somehow', 'the', 'mystery', 'of', 'language', 'was', 'revealed', 'to', 'me', '.', 'i', 'knew', 'then', 'that', 'w', 'a', 't', 'e', 'r', 'meant', 'the', 'wonderful', 'cool', 'something', 'that', 'was', 'flowing', 'over', 'my', 'hand', '.', 'that', 'living', 'word', 'awakened', 'my', 'soul', ',', 'gave', 'it', 'light', ',', 'hope', ',', 'joy', ',', 'set', 'it', 'free', '!', 'there', 'were', 'barriers', 'still', ',', 'it', 'is', 'true', ',', 'but', 'barriers', 'that', 'could', 'in', 'time', 'be', 'swept', 'away', '.']\n"
     ]
    }
   ],
   "source": [
    "corpus = \\\n",
    "\"\"\"\n",
    "in the days that followed i learned to spell in this uncomprehending way a great many words ,  among them pin ,  hat ,  cup and a few verbs like sit ,  stand and walk .  \n",
    "but my teacher had been with me several weeks before i understood that everything has a name . \n",
    "one day ,  we walked down the path to the well house ,  attracted by the fragrance of the honeysuckle with which it was covered .  \n",
    "some one was drawing water and my teacher placed my hand under the spout .  \n",
    "as the cool stream gushed over one hand she spelled into the other the word water ,  first slowly ,  then rapidly .  \n",
    "i stood still ,  my whole attention fixed upon the motions of her fingers .  \n",
    "suddenly i felt a misty consciousness as of something forgotten a thrill of returning thought  and somehow the mystery of language was revealed to me .  \n",
    "i knew then that  w a t e r  meant the wonderful cool something that was flowing over my hand .  \n",
    "that living word awakened my soul ,  gave it light ,  hope ,  joy ,  set it free !  \n",
    "there were barriers still ,  it is true ,  but barriers that could in time be swept away . \n",
    "\"\"\"\n",
    "\n",
    "tokens = corpus.split()\n",
    "\n",
    "print(\"문장이 포함하는 Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da5989c2",
   "metadata": {},
   "source": [
    "### 2) 형태소 기반 토큰화\n",
    "- 한국어 문장은 공백 기준으로 토큰화를 했다간 엉망진창의 단어들이 등장하는 것을 확인 할 수 있음\n",
    "- 이를 해결 하기 위해 형태소 기반 토큰화를 진행함\n",
    "- 한국어 형태소 분석기는 \"KoNLPY\"가 있음\n",
    "    - https://konlpy-ko.readthedocs.io/ko/v0.4.3/  \n",
    "    - 내부적으로 5가지의 형태소 분석 Class를 포함하고 있음\n",
    "-  한국어 형태소 분석기를 사용하는 비교 실험 진행\n",
    "    - 속도 측면에서 뛰어난 분석기 : mecab\n",
    "    - 띄어쓰기나 오탈자에 강건한 분석기 : KOMORAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0fb7250c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum,Kkma,Komoran,Mecab,Okt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3cd93c",
   "metadata": {},
   "source": [
    "**설치 방법**  \n",
    "$ pip install konlpy  \n",
    "\n",
    "$ cd ~/aiffel/text_preprocess  \n",
    "\n",
    "$ git clone https://github.com/SOMJANG/Mecab-ko-for-Google-Colab.git  \n",
    "\n",
    "$ cd Mecab-ko-for-Google-Colab  \n",
    "\n",
    "$ bash install_mecab-ko_on_colab190912.sh  \n",
    "\n",
    "**만약 JVMNotFoundException 오류가 발생한다면**  \n",
    "\n",
    "$ sudo apt update  \n",
    "\n",
    "$ sudo apt install default-jre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "068a7b3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Hannanum] \n",
      "[('코로나바이러스', 'N'), ('는', 'J'), ('2019년', 'N'), ('12월', 'N'), ('중국', 'N'), ('우한', 'N'), ('에서', 'J'), ('처음', 'M'), ('발생', 'N'), ('하', 'X'), ('ㄴ', 'E'), ('뒤', 'N'), ('전', 'N'), ('세계', 'N'), ('로', 'J'), ('확산', 'N'), ('되', 'X'), ('ㄴ', 'E'), (',', 'S'), ('새롭', 'P'), ('은', 'E'), ('유형', 'N'), ('의', 'J'), ('호흡기', 'N'), ('감염', 'N'), ('질환', 'N'), ('이', 'J'), ('ㅂ니다', 'E'), ('.', 'S')]\n"
     ]
    }
   ],
   "source": [
    "tokenizer_list = [Hannanum(),Kkma(),Komoran(),Mecab(),Okt()]\n",
    "\n",
    "kor_text = '코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤 전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.'\n",
    "\n",
    "for tokenizer in tokenizer_list:\n",
    "    print('[{}] \\n{}'.format(tokenizer.__class__.__name__, tokenizer.pos(kor_text)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c716897d",
   "metadata": {},
   "source": [
    "### 3) 사전에 없는 단어의 문제\n",
    "- 공백 기반이나 형태소 기반의 토큰화 기법들은 의미를 가지는 단위로 토큰을 생성함\n",
    "- 이 기법들의 경우 모든 단어를 처리할 수 없기 때문에 자주 등장한 상위 N개의 단어만 사용하고 나머지는 \\<unk>같은 특수 토큰으로 치환함  \n",
    "\n",
    "코로나바이러스는 2019년 12월 중국 우한에서 처음 발생한 뒤\n",
    "전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.\n",
    "\n",
    "→ \\<unk>는 2019년 12월 중국 \\<unk>에서 처음 발생한 뒤\n",
    "전 세계로 확산된, 새로운 유형의 호흡기 감염 질환입니다.  \n",
    "\n",
    "- 위와 같은 현상을 OOV(out-of-vocabualry)문제라고 함\n",
    "- OOV와 같이 새로 등장한 단어에 대해 약한 모습을 해결하고자 Worldpiece Model(WPM)이 등장함\n",
    "- WPM은 pre+view와 pre+dict와 같이 pre(미리, 이전의)의미를 이용해서 한 단어를 여러 개의 subword의 집합으로 보는 방법을 말함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70047b57",
   "metadata": {},
   "source": [
    "### Byte Pair Encoding(BPE)\n",
    "- 데이터에서 가장 많이 등장하는 바이트 쌍(Byte Pair) 을 새로운 단어로 치환하여 압축하는 작업을 반복하는 방식으로 동작\n",
    "-  모든 단어를 문자(바이트)들의 집합으로 취급하여 자주 등장하는 문자 쌍을 합치면, 접두어나 접미어의 의미를 캐치할 수 있고, 처음 등장하는 단어는 문자(알파벳)들의 조합으로 나타내어 OOV 문제를 해결"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde9febf",
   "metadata": {},
   "source": [
    "aaabdaaabac # 가장 많이 등장한 바이트 쌍 \"aa\"를 \"Z\"로 치환합니다.  \n",
    "→   \n",
    "ZabdZabac   # \"aa\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.  \n",
    "Z=aa        # 그다음 많이 등장한 바이트 쌍 \"ab\"를 \"Y\"로 치환합니다.  \n",
    "→   \n",
    "ZYdZYac     # \"ab\" 총 두 개가 치환되어 4바이트를 2바이트로 압축하였습니다.  \n",
    "Z=aa        # 여기서 작업을 멈추어도 되지만, 치환된 바이트에 대해서도 진행한다면  \n",
    "Y=ab        # 가장 많이 등장한 바이트 쌍 \"ZY\"를 \"X\"로 치환합니다.  \n",
    "→   \n",
    "XdXac  \n",
    "Z=aa  \n",
    "Y=ab  \n",
    "X=ZY       # 압축이 완료되었습니다!  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1d2187",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, collections\n",
    "\n",
    "# 임의의 데이터에 포함된 단어들입니다.\n",
    "# 우측의 정수는 임의의 데이터에 해당 단어가 포함된 빈도수입니다.\n",
    "vocab = {\n",
    "    'l o w '      : 5,\n",
    "    'l o w e r '  : 2,\n",
    "    'n e w e s t ': 6,\n",
    "    'w i d e s t ': 3\n",
    "}\n",
    "\n",
    "num_merges = 5\n",
    "\n",
    "def get_stats(vocab):\n",
    "    \"\"\"\n",
    "    단어 사전을 불러와\n",
    "    단어는 공백 단위로 쪼개어 문자 list를 만들고\n",
    "    빈도수와 쌍을 이루게 합니다. (symbols)\n",
    "    \"\"\"\n",
    "    pairs = collections.defaultdict(int)\n",
    "    \n",
    "    for word, freq in vocab.items():\n",
    "        symbols = word.split()\n",
    "\n",
    "        for i in range(len(symbols) - 1):             # 모든 symbols를 확인하여 \n",
    "            pairs[symbols[i], symbols[i + 1]] += freq  # 문자 쌍의 빈도수를 저장합니다. \n",
    "        \n",
    "    return pairs\n",
    "\n",
    "def merge_vocab(pair, v_in):\n",
    "    \"\"\"\n",
    "    문자 쌍(pair)과 단어 리스트(v_in)를 입력받아\n",
    "    각각의 단어에서 등장하는 문자 쌍을 치환합니다.\n",
    "    (하나의 글자처럼 취급)\n",
    "    \"\"\"\n",
    "    v_out = {}\n",
    "    bigram = re.escape(' '.join(pair))\n",
    "    p = re.compile(r'(?<!\\S)' + bigram + r'(?!\\S)')\n",
    "    \n",
    "    for word in v_in:\n",
    "        w_out = p.sub(''.join(pair), word)\n",
    "        v_out[w_out] = v_in[word]\n",
    "        \n",
    "    return v_out, pair[0] + pair[1]\n",
    "\n",
    "token_vocab = []\n",
    "\n",
    "for i in range(num_merges):\n",
    "    print(\">> Step {0}\".format(i + 1))\n",
    "    \n",
    "    pairs = get_stats(vocab)\n",
    "    best = max(pairs, key=pairs.get)  # 가장 많은 빈도수를 가진 문자 쌍을 반환합니다.\n",
    "    vocab, merge_tok = merge_vocab(best, vocab)\n",
    "    print(\"다음 문자 쌍을 치환:\", merge_tok)\n",
    "    print(\"변환된 Vocab:\\n\", vocab, \"\\n\")\n",
    "    \n",
    "    token_vocab.append(merge_tok)\n",
    "    \n",
    "print(\"Merged Vocab:\", token_vocab)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd11d3e9",
   "metadata": {},
   "source": [
    "### Wordpiece Model(WPM)\n",
    "- 구글에서 BPE를 변형해 제안한 알고리즘\n",
    "- BPE에 대해 두 가지 차별성을 가짐\n",
    "    - 공백 복원을 위해 단어의 시작 부분에 언더바 _ 를 추가.\n",
    "    - 빈도수 기반이 아닌 가능도(Likelihood)를 증가시키는 방향으로 문자 쌍을 합침. (더 '그럴듯한' 토큰을 만들어냄)\n",
    "- 조사, 어미 등의 활용이 많고 복잡한 한국어 같은 모델의 토크나이저로 WPM이 좋은 대안이 될 수 있음\n",
    "- WPM은 어떤 언어든 무관하게 적용 가능한 language-neutral하고 general한 기법\n",
    "- 한국어 형태소 분석기처럼 한국어에만 적용 가능한 기법보다 훨씬 활용도가 큼\n",
    "- 구글의 SentencePiece 라이브러리를 통해 사용 가능\n",
    "- SentencePiece에는 전처리 과정도 포함되어 있어서, 데이터를 따로 정제할 필요가 없음\n",
    "- https://github.com/google/sentencepiece\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236b1a24",
   "metadata": {},
   "source": [
    "### soynlp\n",
    "- '학습데이터를 이용하지 않으면서 데이터에 존재하는 단어를 찾거나, 문장을 단어열로 분해, 혹은 품사 판별을 할 수 있는 비지도학습 접근법을 지향합니다'\n",
    "\n",
    "-  soynlp는 한국어 자연어 처리를 위한 라이브러리\n",
    "- 토크나이저 외에도 단어 추출, 품사 판별, 전처리 기능도 제공\n",
    "- 형태소 기반인 koNLPy의 단점을 해결하기 위해 사용\n",
    "- 단어의 경계를 비지도학습을 통해 결정 > 미등록 단어도 토큰화 가능\n",
    "- 통계 기반 토크나이저로 분류하기도 함"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b28fd94",
   "metadata": {},
   "source": [
    "## 토큰에게 의미를 부여하기\n",
    "- 각 토큰들이 랜덤하게 부여된 실수로 살아가지 않게, 그들끼리 유사도 연산을 할 수 있게 의미를 부여하는 알고리즘"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9421586",
   "metadata": {},
   "source": [
    "### Word2Vec\n",
    "- \"단어를 벡터로 만든다\"\n",
    "- 단어 벡터 간의 유의미한 유사도를 반영하여 단어의 의미를 수치화 함\n",
    "- 학습 방식에는 CBOW(Continuous Bag of Words)와 Skip-Gram 두 가지 방식을 이용\n",
    "    - CBOW: 주변의 단어들을 입력으로 중간에 있는 단어들을 예측\n",
    "    - Skip-Gram: 중간에 있는 단어들을 입력으로 주변 단어들을 예측\n",
    "    - Skip-Gram이 실제 실험에서 다소 우세한 경향이 있음\n",
    "- 자주 등장하지 않는 단어는 최악의 경우 단 한 번의 연산만을 거쳐 랜덤하게 초기화된 값과 크게 다르지 않은 상태로 알고리즘이 종료한다는 단점이 있음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45657a8c",
   "metadata": {},
   "source": [
    "### FastText\n",
    "- Word2Vec의 단점을 해결하기 위해 등장\n",
    "- FastText는 한 단어를 n-gram의 집합이라고 보고 단어를 쪼개어 각 n-gram에 할당된 Embedding의 평균값을 사용\n",
    "- https://brunch.co.kr/@learning/7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474d49f1",
   "metadata": {},
   "source": [
    "### ELMo - the 1st Contextualized Word Embedding\n",
    "- 동음이의어를 처리할 수 없다는 단점을 처리하기 위해 등장\n",
    "- 그 단어가 사용된 주변 단어의 맥락을 함께 고려 (contextualized word embedding)\n",
    "- https://brunch.co.kr/@learning/12\n",
    "- ELMo라는 모델은 첫 번째 Contextualized Word Embedding 모델"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
